# final calculation, which gives you theta bar!
2 * theta_hat - bias
}
}
weib_percentile(a_syriaca$doy, percentile = 1, iterations = 100)
weib_percentile <- function(observations, percentile, iterations = 500){
# print warning if trying to estimate at a true bound, because Weibull
# distribution cannot estimate 0th or 100th percentile
if(percentile == 0 | percentile == 1){
print("Weibull distribution cannot estimate value 0th or 1st quantile")
print("please choose a percentile value between 0 and 1")
} else{
# curve_intersect determines where two lines intersect
# parameters needed are two dataframes with two columns, x and y, which could
# be plotted.
curve_intersect <- function(curve1, curve2, empirical=TRUE, domain=NULL) {
if (!empirical & missing(domain)) {
stop("'domain' must be provided with non-empirical curves")
}
if (!empirical & (length(domain) != 2 | !is.numeric(domain))) {
stop("'domain' must be a two-value numeric vector, like c(0, 10)")
}
if (empirical) {
# Approximate the functional form of both curves
curve1_f <- stats::approxfun(curve1$x, curve1$y, rule = 2)
curve2_f <- suppressWarnings(stats::approxfun(curve2$x, curve2$y, rule = 2))
# Calculate the intersection of curve 1 and curve 2 along the x-axis
point_x <- stats::uniroot(function(x) curve1_f(x) - curve2_f(x),
c(min(curve1$x), max(curve1$x)))$root
# Find where point_x is in curve 2
point_y <- curve2_f(point_x)
} else {
# Calculate the intersection of curve 1 and curve 2 along the x-axis
# within the given domain
point_x <- stats::uniroot(function(x) curve1(x) - curve2(x), domain)$root
# Find where point_x is in curve 2
point_y <- curve2(point_x)
}
return(list(x = point_x, y = point_y))
}
# use a data frame to plot a smooth CDF from -0.01 to 1.01 given
# our original observations
create_predict_df <- function(observations){
# previous create_cdf_ends()
weib <- fitdistrplus::fitdist(observations, distr = "weibull",
method = "mle")
cdf0 <- as.numeric(weib$estimate['scale']*
(-log(1-0.01))^(1/weib$estimate['shape']))
cdf100 <- as.numeric(weib$estimate['scale']*
(-log(1-0.99))^(1/weib$estimate['shape']))
added_vec <- sort(append(observations, values = c(cdf0, cdf100)),
decreasing = FALSE)
new_vec <- seq(from = min(added_vec), to = max(added_vec), by = 0.5)
cdfadded <- 1 - exp(-(new_vec/weib$estimate['scale'])^weib$estimate['shape'])
cdf_df <- data.frame(x = new_vec, y = cdfadded)
ends <- data.frame(x = c(min(added_vec - 1), max(added_vec + 1)),
y = c(-0.001,1.001))
cdf_df <- rbind(cdf_df, ends)
cdf_df <- cdf_df[order(cdf_df$x, decreasing = FALSE),]
return(cdf_df)
}
# calculates the theta hat value for each iteration, which
# when averaged is used to calculate the bias value
get_theta_hat_i <- function(observations, percentile){
emptyvec <- vector(mode = "numeric", length = length(observations))
df1 <- create_predict_df(observations) # move out of loop since constant
for(i in seq_along(observations)){
sim_vector <- stats::runif(n = 1, min = 0, max = 1)
df2 <- data.frame(x = observations, y = sim_vector)
emptyvec[i] <- curve_intersect(df1, df2)$x
}
new_vector <- sort(emptyvec, decreasing = FALSE)
new_df1 <- create_predict_df(new_vector)
new_df2 <- data.frame(x = observations, y = percentile)
theta_hat_i <- curve_intersect(new_df1, new_df2)$x
return(theta_hat_i)
}
theta_hat_df <- data.frame(x = observations, y = percentile)
# calculate theta hat original value
theta_hat <- curve_intersect(curve1 = create_predict_df(observations),
curve2 = theta_hat_df)[['x']]
# calculate bias value based off of the mean of many theta hat i calculations
bias <- mean(replicate(n = iterations,
expr = get_theta_hat_i(observations = observations,
percentile = percentile)))
# final calculation, which gives you theta bar!
2 * theta_hat - bias
}
}
weib_percentile(a_syriaca$doy, percentile = 1, iterations = 100)
weib_percentile(a_syriaca$doy, percentile = 0, iterations = 100)
weib_percentile(a_syriaca$doy, percentile = 0.00001, iterations = 100)
weib_percentile(a_syriaca$doy, percentile = 0.99999, iterations = 100)
#' a_syriaca <- subset(inat_examples, scientific_name == "Asclepias syriaca")
#' weib_percentile(a_syriaca$doy, percentile = 0.5, iterations = 500)
#'
#' # Estimate when 90 percent of individuals of the milkweed species A. syriaca
#' # have been observed, using only 100 iterations for quicker processing. To get
#' # more stable result, more iterations should be used.
#'
#' weib_percentile(a_syriaca$doy, percentile = 0.9, iterations = 100)
#' }
#'
weib_percentile <- function(observations, percentile, iterations = 500){
# print warning if trying to estimate at a true bound, because Weibull
# distribution cannot estimate 0th or 100th percentile
if(percentile == 0 | percentile == 1){
print("Weibull distribution cannot estimate value 0th or 1st quantile")
print("please choose a percentile value between 0 and 1")
} else{
# curve_intersect determines where two lines intersect
# parameters needed are two dataframes with two columns, x and y, which could
# be plotted.
curve_intersect <- function(curve1, curve2, empirical=TRUE, domain=NULL) {
if (!empirical & missing(domain)) {
stop("'domain' must be provided with non-empirical curves")
}
if (!empirical & (length(domain) != 2 | !is.numeric(domain))) {
stop("'domain' must be a two-value numeric vector, like c(0, 10)")
}
if (empirical) {
# Approximate the functional form of both curves
curve1_f <- stats::approxfun(curve1$x, curve1$y, rule = 2)
curve2_f <- suppressWarnings(stats::approxfun(curve2$x, curve2$y, rule = 2))
# Calculate the intersection of curve 1 and curve 2 along the x-axis
point_x <- stats::uniroot(function(x) curve1_f(x) - curve2_f(x),
c(min(curve1$x), max(curve1$x)))$root
# Find where point_x is in curve 2
point_y <- curve2_f(point_x)
} else {
# Calculate the intersection of curve 1 and curve 2 along the x-axis
# within the given domain
point_x <- stats::uniroot(function(x) curve1(x) - curve2(x), domain)$root
# Find where point_x is in curve 2
point_y <- curve2(point_x)
}
return(list(x = point_x, y = point_y))
}
# use a data frame to plot a smooth CDF from -0.01 to 1.01 given
# our original observations
create_predict_df <- function(observations){
# previous create_cdf_ends()
weib <- fitdistrplus::fitdist(observations, distr = "weibull",
method = "mle")
cdf0 <- as.numeric(weib$estimate['scale']*
(-log(1-0.01))^(1/weib$estimate['shape']))
cdf100 <- as.numeric(weib$estimate['scale']*
(-log(1-0.99))^(1/weib$estimate['shape']))
added_vec <- sort(append(observations, values = c(cdf0, cdf100)),
decreasing = FALSE)
new_vec <- seq(from = min(added_vec), to = max(added_vec), by = 0.5)
cdfadded <- 1 - exp(-(new_vec/weib$estimate['scale'])^weib$estimate['shape'])
cdf_df <- data.frame(x = new_vec, y = cdfadded)
ends <- data.frame(x = c(min(added_vec - 1), max(added_vec + 1)),
y = c(-0.001,1.001))
cdf_df <- rbind(cdf_df, ends)
cdf_df <- cdf_df[order(cdf_df$x, decreasing = FALSE),]
return(cdf_df)
}
# calculates the theta hat value for each iteration, which
# when averaged is used to calculate the bias value
get_theta_hat_i <- function(observations, percentile){
emptyvec <- vector(mode = "numeric", length = length(observations))
df1 <- create_predict_df(observations) # move out of loop since constant
for(i in seq_along(observations)){
sim_vector <- stats::runif(n = 1, min = 0, max = 1)
df2 <- data.frame(x = observations, y = sim_vector)
emptyvec[i] <- curve_intersect(df1, df2)$x
}
new_vector <- sort(emptyvec, decreasing = FALSE)
new_df1 <- create_predict_df(new_vector)
new_df2 <- data.frame(x = observations, y = percentile)
theta_hat_i <- curve_intersect(new_df1, new_df2)$x
return(theta_hat_i)
}
theta_hat_df <- data.frame(x = observations, y = percentile)
# calculate theta hat original value
theta_hat <- curve_intersect(curve1 = create_predict_df(observations),
curve2 = theta_hat_df)[['x']]
# calculate bias value based off of the mean of many theta hat i calculations
bias <- mean(replicate(n = iterations,
expr = get_theta_hat_i(observations = observations,
percentile = percentile)))
# final calculation, which gives you theta bar!
2 * theta_hat - bias
}
}
weib_percentile(a_syriaca$doy, percentile = 0.9, iterations = 100)
weib_percentile(a_syriaca$doy, percentile = 0.5, iterations = 500)
#' a_syriaca <- subset(inat_examples, scientific_name == "Asclepias syriaca")
#' weib_percentile(a_syriaca$doy, percentile = 0.5, iterations = 500)
#'
#' # Estimate when 90 percent of individuals of the milkweed species A. syriaca
#' # have been observed, using only 100 iterations for quicker processing. To
#' # get a more stable result, more iterations should be used.
#'
#' weib_percentile(a_syriaca$doy, percentile = 0.9, iterations = 100)
#' }
#'
weib_percentile <- function(observations, percentile, iterations = 500){
# print warning if trying to estimate at a true bound, because Weibull
# distribution cannot estimate 0th or 100th percentile
if(percentile == 0 | percentile == 1){
print("Weibull distribution cannot estimate value 0th or 1st quantile")
print("please choose a percentile value between 0 and 1")
} else{
# curve_intersect determines where two lines intersect
# parameters needed are two dataframes with two columns, x and y, which could
# be plotted.
curve_intersect <- function(curve1, curve2, empirical=TRUE, domain=NULL) {
if (!empirical & missing(domain)) {
stop("'domain' must be provided with non-empirical curves")
}
if (!empirical & (length(domain) != 2 | !is.numeric(domain))) {
stop("'domain' must be a two-value numeric vector, like c(0, 10)")
}
if (empirical) {
# Approximate the functional form of both curves
curve1_f <- stats::approxfun(curve1$x, curve1$y, rule = 2)
curve2_f <- suppressWarnings(stats::approxfun(curve2$x, curve2$y, rule = 2))
# Calculate the intersection of curve 1 and curve 2 along the x-axis
point_x <- stats::uniroot(function(x) curve1_f(x) - curve2_f(x),
c(min(curve1$x), max(curve1$x)))$root
# Find where point_x is in curve 2
point_y <- curve2_f(point_x)
} else {
# Calculate the intersection of curve 1 and curve 2 along the x-axis
# within the given domain
point_x <- stats::uniroot(function(x) curve1(x) - curve2(x), domain)$root
# Find where point_x is in curve 2
point_y <- curve2(point_x)
}
return(list(x = point_x, y = point_y))
}
# use a data frame to plot a smooth CDF from -0.01 to 1.01 given
# our original observations
create_predict_df <- function(observations){
# previous create_cdf_ends()
weib <- fitdistrplus::fitdist(observations, distr = "weibull",
method = "mle")
cdf0 <- as.numeric(weib$estimate['scale']*
(-log(1-0.01))^(1/weib$estimate['shape']))
cdf100 <- as.numeric(weib$estimate['scale']*
(-log(1-0.99))^(1/weib$estimate['shape']))
added_vec <- sort(append(observations, values = c(cdf0, cdf100)),
decreasing = FALSE)
new_vec <- seq(from = min(added_vec), to = max(added_vec), by = 0.5)
cdfadded <- 1 - exp(-(new_vec/weib$estimate['scale'])^weib$estimate['shape'])
cdf_df <- data.frame(x = new_vec, y = cdfadded)
ends <- data.frame(x = c(min(added_vec - 1), max(added_vec + 1)),
y = c(-0.001,1.001))
cdf_df <- rbind(cdf_df, ends)
cdf_df <- cdf_df[order(cdf_df$x, decreasing = FALSE),]
return(cdf_df)
}
# calculates the theta hat value for each iteration, which
# when averaged is used to calculate the bias value
get_theta_hat_i <- function(observations, percentile){
emptyvec <- vector(mode = "numeric", length = length(observations))
df1 <- create_predict_df(observations) # move out of loop since constant
for(i in seq_along(observations)){
sim_vector <- stats::runif(n = 1, min = 0, max = 1)
df2 <- data.frame(x = observations, y = sim_vector)
emptyvec[i] <- curve_intersect(df1, df2)$x
}
new_vector <- sort(emptyvec, decreasing = FALSE)
new_df1 <- create_predict_df(new_vector)
new_df2 <- data.frame(x = observations, y = percentile)
theta_hat_i <- curve_intersect(new_df1, new_df2)$x
return(theta_hat_i)
}
theta_hat_df <- data.frame(x = observations, y = percentile)
# calculate theta hat original value
theta_hat <- curve_intersect(curve1 = create_predict_df(observations),
curve2 = theta_hat_df)[['x']]
# calculate bias value based off of the mean of many theta hat i calculations
bias <- mean(replicate(n = iterations,
expr = get_theta_hat_i(observations = observations,
percentile = percentile)))
# final calculation, which gives you theta bar!
2 * theta_hat - bias
}
}
#'
#'\donttest{
#' data(inat_examples)
#' s_cybele <- subset(inat_examples, scientific_name == "Speyeria cybele")
#' weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
#'                    percentile = 0.5, bootstraps = 10)
#' }
#'
#'@export
#'
weib_percentile_ci <- function(observations, iterations, percentile, bootstraps,
type = "bca", conf = 0.95, parallelize = "no",
ncpus = getOption("boot.ncpus", 1L), cl = NULL){
weibfun <- function(data, i){
d <- data[i]
return(weib_percentile(d, iterations = iterations, percentile = percentile))
}
estimate_ci(observations, .f = weibfun, n_boots = bootstraps,
conf = conf, type = type, parallelize = parallelize,
ncpus = ncpus, cl = cl)
}
#' a_syriaca <- subset(inat_examples, scientific_name == "Asclepias syriaca")
#' weib_percentile(a_syriaca$doy, percentile = 0.5, iterations = 500)
#'
#' # Estimate when 90 percent of individuals of the milkweed species A. syriaca
#' # have been observed, using only 100 iterations for quicker processing. To
#' # get a more stable result, more iterations should be used.
#'
#' weib_percentile(a_syriaca$doy, percentile = 0.9, iterations = 100)
#' }
#'
weib_percentile <- function(observations, percentile, iterations = 500){
# print warning if trying to estimate at a true bound, because Weibull
# distribution cannot estimate 0th or 100th percentile
if(percentile == 0 | percentile == 1){
print("Weibull distribution cannot estimate value 0th or 1st quantile")
print("please choose a percentile value between 0 and 1")
} else{
# curve_intersect determines where two lines intersect
# parameters needed are two dataframes with two columns, x and y, which could
# be plotted.
curve_intersect <- function(curve1, curve2, empirical=TRUE, domain=NULL) {
if (!empirical & missing(domain)) {
stop("'domain' must be provided with non-empirical curves")
}
if (!empirical & (length(domain) != 2 | !is.numeric(domain))) {
stop("'domain' must be a two-value numeric vector, like c(0, 10)")
}
if (empirical) {
# Approximate the functional form of both curves
curve1_f <- stats::approxfun(curve1$x, curve1$y, rule = 2)
curve2_f <- suppressWarnings(stats::approxfun(curve2$x, curve2$y, rule = 2))
# Calculate the intersection of curve 1 and curve 2 along the x-axis
point_x <- stats::uniroot(function(x) curve1_f(x) - curve2_f(x),
c(min(curve1$x), max(curve1$x)))$root
# Find where point_x is in curve 2
point_y <- curve2_f(point_x)
} else {
# Calculate the intersection of curve 1 and curve 2 along the x-axis
# within the given domain
point_x <- stats::uniroot(function(x) curve1(x) - curve2(x), domain)$root
# Find where point_x is in curve 2
point_y <- curve2(point_x)
}
return(list(x = point_x, y = point_y))
}
# use a data frame to plot a smooth CDF from -0.01 to 1.01 given
# our original observations
create_predict_df <- function(observations){
# previous create_cdf_ends()
weib <- fitdistrplus::fitdist(observations, distr = "weibull",
method = "mle")
cdf0 <- as.numeric(weib$estimate['scale']*
(-log(1-0.01))^(1/weib$estimate['shape']))
cdf100 <- as.numeric(weib$estimate['scale']*
(-log(1-0.99))^(1/weib$estimate['shape']))
added_vec <- sort(append(observations, values = c(cdf0, cdf100)),
decreasing = FALSE)
new_vec <- seq(from = min(added_vec), to = max(added_vec), by = 0.5)
cdfadded <- 1 - exp(-(new_vec/weib$estimate['scale'])^weib$estimate['shape'])
cdf_df <- data.frame(x = new_vec, y = cdfadded)
ends <- data.frame(x = c(min(added_vec - 1), max(added_vec + 1)),
y = c(-0.001,1.001))
cdf_df <- rbind(cdf_df, ends)
cdf_df <- cdf_df[order(cdf_df$x, decreasing = FALSE),]
return(cdf_df)
}
# calculates the theta hat value for each iteration, which
# when averaged is used to calculate the bias value
get_theta_hat_i <- function(observations, percentile){
emptyvec <- vector(mode = "numeric", length = length(observations))
df1 <- create_predict_df(observations) # move out of loop since constant
for(i in seq_along(observations)){
sim_vector <- stats::runif(n = 1, min = 0, max = 1)
df2 <- data.frame(x = observations, y = sim_vector)
emptyvec[i] <- curve_intersect(df1, df2)$x
}
new_vector <- sort(emptyvec, decreasing = FALSE)
new_df1 <- create_predict_df(new_vector)
new_df2 <- data.frame(x = observations, y = percentile)
theta_hat_i <- curve_intersect(new_df1, new_df2)$x
return(theta_hat_i)
}
theta_hat_df <- data.frame(x = observations, y = percentile)
# calculate theta hat original value
theta_hat <- curve_intersect(curve1 = create_predict_df(observations),
curve2 = theta_hat_df)[['x']]
# calculate bias value based off of the mean of many theta hat i calculations
bias <- mean(replicate(n = iterations,
expr = get_theta_hat_i(observations = observations,
percentile = percentile)))
# final calculation, which gives you theta bar!
2 * theta_hat - bias
}
}
data(inat_examples)
s_cybele <- subset(inat_examples, scientific_name == "Speyeria cybele")
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.5, bootstraps = 10)
#' @param parallelize The type of parallel operation to be used (if any). If
#' missing, the default is that no parallelization will occur. Parallelization
#' options are "multicore" and "snow"
#' @param ncpus An integer that represents the number of processes to be
#' used in parallel operation.
#' @param cl An optional parallel or snow cluster for use if parallel = "snow".
#' If not supplied, a cluster on the local machine is created for
#' the duration of the boot call.
#' @return A data frame with estimate, and the lower and upper points of its confidence interval
#' @export
estimate_ci <- function(observations, .f, n_boots,
parallelize = "no",
ncpus = getOption("boot.ncpus", 1L),
cl = NULL,
type = "bca", conf = 0.95){
bootstrap <- boot::boot(observations, .f, R = n_boots,
parallel = parallelize,
ncpus = ncpus, cl = cl)
boot_ci <- tryCatch(boot::boot.ci(bootstrap, conf = conf, type = type),
error = function(e) NA)
if(type == "bca"){
low_ci <- boot_ci$bca[4]
high_ci <- boot_ci$bca[5]
} else if(type == "perc"){
low_ci <-boot_ci$percent[4]
high_ci <- boot_ci$percent[5]
} else if(type == "norm"){
low_ci <- boot_ci$normal[4]
high_ci <- boot_ci$normal[5]
} else if(type == "basic"){
low_ci <- boot_ci$basic[4]
high_ci <- boot_ci$basic[5]
} else{
low_ci <- "Bootstrap type NA"
high_ci <- "Bootstrap type NA"
}
data.frame(estimate = bootstrap$t0, low_ci, high_ci)
}
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.5, bootstraps = 10)
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.5, bootstraps = 100)
#'
#'\donttest{
#' data(inat_examples)
#' s_cybele <- subset(inat_examples, scientific_name == "Speyeria cybele")
#' weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
#'                    percentile = 0.5, bootstraps = 100)
#' }
#'
#'@export
#'
weib_percentile_ci <- function(observations, iterations, percentile, bootstraps,
type = "perc", conf = 0.95, parallelize = "no",
ncpus = getOption("boot.ncpus", 1L), cl = NULL){
weibfun <- function(data, i){
d <- data[i]
return(weib_percentile(d, iterations = iterations, percentile = percentile))
}
estimate_ci(observations, .f = weibfun, n_boots = bootstraps,
conf = conf, type = type, parallelize = parallelize,
ncpus = ncpus, cl = cl)
}
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.5, bootstraps = 100)
library(phenesse)
?weib_percentile_ci
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.5, bootstraps = 10)
data(inat_examples)
r_hirta <- subset(inat_examples, scientific_name == "Rudbeckia hirta")
mean_ci(observations = r_hirta$doy , bootstraps = 100)
data(inat_examples)
s_cybele <- subset(inat_examples, scientific_name == "Speyeria cybele")
quantile_ci(observations = s_cybele$doy, percentile = 0.1, bootstraps = 100)
dir.create("inst")
?weib_percentile
knitr::opts_chunk$set(
cache = FALSE,
collapse = TRUE,
comment = "#>"
)
library(phenesse)
s_cybele <- subset(inat_examples, scientific_name == "Speyeria cybele")
# calculate onset, we're using very low iterations and bootstraps to knit vignette quickly. Please increase both iterations and bootstraps
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.01, bootstraps = 100)
# note warnings of extreme order points used as endpoints is due to the
# low number of bootsraps used. Please use higher number of bootstraps if
# using for analyses.
?weib_percentile_ci
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.01, bootstraps = 100, parallelize = "yes", ncpus = 4)
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.01, bootstraps = 100, parallelize = "yes", ncpus = 4, cl = "snow")
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.01, bootstraps = 100, parallelize = "snow", ncpus = 4, cl = "snow")
weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.01, bootstraps = 100, parallelize = "multicore", ncpus = 4)
system.time(weib_percentile_ci(observations = s_cybele$doy, iterations = 10,
percentile = 0.01, bootstraps = 100, parallelize = "multicore", ncpus = 4))
?mclapply
inat_examples <- inat_examples
library(devtools)
document()
document()
document()
check()
?build
build()
